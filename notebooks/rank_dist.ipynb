{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rank_dist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPSaTtAknNoxvk2wyDnw39f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanjeffares/Simplex/blob/main/notebooks/rank_dist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alanjeffares/Simplex.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaGHsss8Ahsh",
        "outputId": "004b4901-2704-4cd1-ef7d-9e1c8f2c7fea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Simplex'...\n",
            "remote: Enumerating objects: 625, done.\u001b[K\n",
            "remote: Counting objects: 100% (625/625), done.\u001b[K\n",
            "remote: Compressing objects: 100% (421/421), done.\u001b[K\n",
            "remote: Total 625 (delta 365), reused 395 (delta 153), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (625/625), 1.35 MiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (365/365), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum\n",
        "!pip install pytorch_influence_functions\n",
        "\n",
        "\n",
        "# science plots requirements\n",
        "!apt-get update\n",
        "!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended cm-super\n",
        "!pip install SciencePlots\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.reload_library()\n",
        "plt.style.use('science')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QtSSHMKoCNU",
        "outputId": "00145e01-6ba8-4715-cfe9-bc41f3691c64"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: captum in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.10.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n",
            "Requirement already satisfied: pytorch_influence_functions in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_influence_functions) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_influence_functions) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->pytorch_influence_functions) (3.10.0.2)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.4 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Hit:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,517 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.7 MB in 8s (1,941 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal fonts-droid-fallback fonts-lato fonts-lmodern\n",
            "  fonts-noto-mono fonts-texgyre ghostscript gsfonts javascript-common\n",
            "  libcupsfilters1 libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0\n",
            "  libjs-jquery libkpathsea6 libpotrace0 libptexenc1 libruby2.5 libsynctex1\n",
            "  libtexlua52 libtexluajit2 libzzip-0-13 lmodern pfb2t1c2pfb poppler-data\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-latex-base texlive-latex-recommended\n",
            "  texlive-pictures texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  fonts-noto ghostscript-x apache2 | lighttpd | httpd poppler-utils\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri\n",
            "  ruby-dev bundler debhelper perl-tk xpdf-reader | pdf-viewer\n",
            "  texlive-fonts-recommended-doc texlive-latex-base-doc python-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl\n",
            "  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-pstricks\n",
            "  dot2tex prerex ruby-tcltk | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal dvipng fonts-droid-fallback fonts-lato\n",
            "  fonts-lmodern fonts-noto-mono fonts-texgyre ghostscript gsfonts\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  pfb2t1c2pfb poppler-data preview-latex-style rake ruby ruby-did-you-mean\n",
            "  ruby-minitest ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-latex-base\n",
            "  texlive-latex-extra texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa\n",
            "0 upgraded, 51 newly installed, 0 to remove and 70 not upgraded.\n",
            "Need to get 163 MB of archives.\n",
            "After this operation, 503 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.15 [5,092 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [2,265 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [51.4 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.11 [48.6 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.11 [3,072 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 163 MB in 17s (9,556 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 51.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../04-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../05-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../06-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../07-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../08-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../09-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../10-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../11-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../12-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../13-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.15_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../14-libgs9_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../15-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../16-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../17-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../18-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../19-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../20-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../21-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "Preparing to unpack .../22-cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../23-pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../24-cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../25-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../26-dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../27-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../28-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../29-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../30-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../31-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../32-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../33-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../34-ruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../35-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../36-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../37-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../38-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../39-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../40-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../41-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../42-libruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../43-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../44-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../46-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../47-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../48-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../49-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../50-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Collecting SciencePlots\n",
            "  Downloading SciencePlots-1.0.9.tar.gz (10 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for SciencePlots from https://files.pythonhosted.org/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz#sha256=2b002f263734a718acdf1e5be57f0de5ee887e70517dfbe1118a27d0ee3dfb0f.\u001b[0m\n",
            "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from SciencePlots) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SciencePlots) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SciencePlots) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SciencePlots) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SciencePlots) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SciencePlots) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->SciencePlots) (1.15.0)\n",
            "Building wheels for collected packages: SciencePlots\n",
            "  Building wheel for SciencePlots (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SciencePlots: filename=SciencePlots-1.0.9-py3-none-any.whl size=6484 sha256=2229437f87723cfb071190259238e5ef586121b24401ee55eaef0d72a464daee\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/f8/e9/b2f53a40b336388dfc57b108150daff7d6ffbbfc618dba3924\n",
            "Successfully built SciencePlots\n",
            "Installing collected packages: SciencePlots\n",
            "Successfully installed SciencePlots-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Simplex/\n",
        "import explainers\n",
        "from explainers.simplex import Simplex\n",
        "import models\n",
        "from models.image_recognition import MnistClassifier\n",
        "from utils.schedulers import ExponentialScheduler\n",
        "from experiments.mnist import load_mnist\n",
        "%cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYwGfV-Ms-BA",
        "outputId": "b3315b56-f864-4e99-d0ad-a4e2e544cce6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Simplex\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "corpus_size = 100\n",
        "cv_ls = list(range(10))"
      ],
      "metadata": {
        "id": "an9U-_2JYLFo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code\n",
        "%cd Simplex/\n",
        "for cv in cv_ls:\n",
        "  !python -m experiments.mnist -experiment \"approximation_quality\" -cv $cv -corpus_size $corpus_size\n",
        "%cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOACcrMIs2_y",
        "outputId": "0b7c291d-a333-4de2-8e6a-c3500c64a69f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Simplex\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 0.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "9913344it [00:00, 23075971.88it/s]                 \n",
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "29696it [00:00, 86495869.16it/s]\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "1649664it [00:00, 7459579.15it/s]                 \n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "5120it [00:00, 26381863.00it/s]\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.3168, Accuracy: 664/10000(7%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.259823\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.266444\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.130640\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.129814\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.925091\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.708768\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.775491\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.572640\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.661589\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.926909\n",
            "\n",
            "Test set: Avg. loss: 0.3928, Accuracy: 9033/10000(90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.704346\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.442015\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.654928\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.629924\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.748165\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.693997\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.575841\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.745505\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.514621\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.563954\n",
            "\n",
            "Test set: Avg. loss: 0.3396, Accuracy: 9178/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.475771\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.671556\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.531803\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.533031\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.449644\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.582352\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.703937\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.807206\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.634571\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.514541\n",
            "\n",
            "Test set: Avg. loss: 0.3006, Accuracy: 9265/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.558825\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.524883\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.645310\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.724440\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.516274\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.483648\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.369184\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.487806\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.653961\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.620299\n",
            "\n",
            "Test set: Avg. loss: 0.3138, Accuracy: 9282/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.456991\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.495168\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.760891\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.540031\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.470800\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.637337\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.563121\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.396820\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.565291\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.528713\n",
            "\n",
            "Test set: Avg. loss: 0.3043, Accuracy: 9252/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.658116\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.683152\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.483591\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.471848\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.517870\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.566064\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.667746\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.564268\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.470993\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.738555\n",
            "\n",
            "Test set: Avg. loss: 0.2866, Accuracy: 9270/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.426937\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.515999\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.445260\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.525281\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.546878\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.284387\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.540394\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.577730\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.477496\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.419833\n",
            "\n",
            "Test set: Avg. loss: 0.2826, Accuracy: 9305/10000(93%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.611588\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.610396\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.440506\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.506458\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.514108\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.410112\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.442638\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.647006\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.480421\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.592603\n",
            "\n",
            "Test set: Avg. loss: 0.2823, Accuracy: 9341/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.502806\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.471115\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.367892\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.515233\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.528471\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.330753\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.564772\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.534058\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.466873\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.453261\n",
            "\n",
            "Test set: Avg. loss: 0.2883, Accuracy: 9293/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.383590\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.409656\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.474869\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.417317\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.518749\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.645900\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.472407\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.539577\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.661309\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.718765\n",
            "\n",
            "Test set: Avg. loss: 0.2895, Accuracy: 9281/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.28e+03 ; Regulator: 76.5 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.74e+03 ; Regulator: 41.9 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.32e+03 ; Regulator: 19.8 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.27e+03 ; Regulator: 5.42 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.34e+03 ; Regulator: 0.609 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv0_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv0_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv0_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv0.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv0.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.95.\n",
            "nn_uniform latent r2: 0.85 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.27e+03 ; Regulator: 63.6 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.74e+03 ; Regulator: 27.8 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.31e+03 ; Regulator: 10.2 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.23e+03 ; Regulator: 2.39 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.21e+03 ; Regulator: 0.382 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv0_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv0_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv0_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv0.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv0.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.84 ; output r2 = 0.88.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.27e+03 ; Regulator: 41.2 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.74e+03 ; Regulator: 13.3 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.32e+03 ; Regulator: 4.52 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.21e+03 ; Regulator: 1.28 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.18e+03 ; Regulator: 0.268 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv0_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv0_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv0_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv0.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv0.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.8 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.83 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.28e+03 ; Regulator: 22.4 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.75e+03 ; Regulator: 6.71 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.32e+03 ; Regulator: 2.43 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.21e+03 ; Regulator: 0.819 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.18e+03 ; Regulator: 0.2 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv0_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv0_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv0_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv0.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv0.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.67 ; output r2 = 0.72.\n",
            "nn_dist latent r2: 0.75 ; output r2 = 0.8.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.28e+03 ; Regulator: 9.51 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.75e+03 ; Regulator: 2.97 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.32e+03 ; Regulator: 1.11 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.22e+03 ; Regulator: 0.404 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.18e+03 ; Regulator: 0.109 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv0_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv0_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv0_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv0.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv0.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.39 ; output r2 = 0.45.\n",
            "nn_dist latent r2: 0.54 ; output r2 = 0.6.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv0.pkl.\n",
            "representer output r2 = -15.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 1.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3152, Accuracy: 1094/10000(11%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333745\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.212021\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.704261\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.982853\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.934310\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.953470\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.674683\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.582133\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.672293\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.795328\n",
            "\n",
            "Test set: Avg. loss: 0.3820, Accuracy: 9093/10000(91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.585360\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.625809\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.673421\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.756576\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.508456\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.436237\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.675758\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.479216\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.625136\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.477917\n",
            "\n",
            "Test set: Avg. loss: 0.3264, Accuracy: 9229/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.617293\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.684406\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.641877\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.471989\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.341410\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.621246\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.529533\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.529505\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.557811\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.647223\n",
            "\n",
            "Test set: Avg. loss: 0.2912, Accuracy: 9309/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.552025\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.373537\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.596081\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.622880\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.574722\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.328787\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.488166\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.425989\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.445913\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.573122\n",
            "\n",
            "Test set: Avg. loss: 0.2893, Accuracy: 9354/10000(94%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.720331\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.392460\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.488729\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.747960\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.425219\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.683693\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.510310\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.527179\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.607146\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.499490\n",
            "\n",
            "Test set: Avg. loss: 0.2960, Accuracy: 9301/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.558351\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.491045\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.568635\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.366363\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.656690\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.591309\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.551832\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.658820\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.571349\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.459486\n",
            "\n",
            "Test set: Avg. loss: 0.2836, Accuracy: 9309/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.535330\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.533469\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.381001\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.376440\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.376183\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.351087\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.663535\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.395716\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.553524\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.335467\n",
            "\n",
            "Test set: Avg. loss: 0.2880, Accuracy: 9328/10000(93%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.671141\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.541204\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.512658\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.487882\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.550578\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.415844\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.559435\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.513552\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.358293\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.652407\n",
            "\n",
            "Test set: Avg. loss: 0.2744, Accuracy: 9375/10000(94%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.348648\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.408585\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.456705\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.539296\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.438871\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.496684\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.521280\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.433705\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.536860\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.442350\n",
            "\n",
            "Test set: Avg. loss: 0.2620, Accuracy: 9388/10000(94%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.600153\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.504153\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.589459\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.527121\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.485426\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.479025\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.600894\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.407130\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.474458\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.509280\n",
            "\n",
            "Test set: Avg. loss: 0.2607, Accuracy: 9384/10000(94%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 76.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.48e+03 ; Regulator: 43.4 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.06e+03 ; Regulator: 19 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.01e+03 ; Regulator: 5.52 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.08e+03 ; Regulator: 0.792 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv1_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv1_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv1_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv1.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv1.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.88 ; output r2 = 0.91.\n",
            "nn_dist latent r2: 0.88 ; output r2 = 0.92.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 64.3 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.48e+03 ; Regulator: 29.3 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.06e+03 ; Regulator: 10.6 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 971 ; Regulator: 2.49 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 951 ; Regulator: 0.402 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv1_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv1_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv1_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv1.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv1.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.95 ; output r2 = 0.98.\n",
            "nn_uniform latent r2: 0.86 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.87 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 42.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.49e+03 ; Regulator: 14.8 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.06e+03 ; Regulator: 4.97 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 956 ; Regulator: 1.39 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 922 ; Regulator: 0.29 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv1_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv1_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv1_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv1.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv1.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.95 ; output r2 = 0.98.\n",
            "nn_uniform latent r2: 0.79 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.84 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 22.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.49e+03 ; Regulator: 7.02 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.06e+03 ; Regulator: 2.59 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 956 ; Regulator: 0.867 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 919 ; Regulator: 0.212 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv1_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv1_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv1_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv1.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv1.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.95 ; output r2 = 0.98.\n",
            "nn_uniform latent r2: 0.66 ; output r2 = 0.73.\n",
            "nn_dist latent r2: 0.75 ; output r2 = 0.81.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.11e+03 ; Regulator: 9.53 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.49e+03 ; Regulator: 3.07 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.06e+03 ; Regulator: 1.18 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 958 ; Regulator: 0.428 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 921 ; Regulator: 0.116 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv1_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv1_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv1_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv1.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv1.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.95 ; output r2 = 0.98.\n",
            "nn_uniform latent r2: 0.39 ; output r2 = 0.45.\n",
            "nn_dist latent r2: 0.55 ; output r2 = 0.61.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv1.pkl.\n",
            "representer output r2 = -23.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 2.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3200, Accuracy: 792/10000(8%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.386671\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.178877\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.412541\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.000690\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.837503\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.950810\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.605311\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.694912\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.764258\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.557235\n",
            "\n",
            "Test set: Avg. loss: 0.4199, Accuracy: 9039/10000(90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.810050\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.591751\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.812688\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.615550\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.580006\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.732461\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.423038\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.645833\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.573131\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.525519\n",
            "\n",
            "Test set: Avg. loss: 0.3312, Accuracy: 9120/10000(91%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.547247\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.503445\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.757753\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.638446\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.571490\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.515759\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.807272\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.518995\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.549182\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.625898\n",
            "\n",
            "Test set: Avg. loss: 0.3168, Accuracy: 9248/10000(92%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.553809\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.645000\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.520735\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.661924\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.490266\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.584657\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.536261\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.764354\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.908667\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.608776\n",
            "\n",
            "Test set: Avg. loss: 0.3118, Accuracy: 9243/10000(92%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.574459\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.719448\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.466770\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.539783\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.570983\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.392059\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.575616\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.657062\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.528261\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.731531\n",
            "\n",
            "Test set: Avg. loss: 0.3015, Accuracy: 9293/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.759285\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.609536\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.430988\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.528916\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.562803\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.561894\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.488010\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.504732\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.447469\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.474839\n",
            "\n",
            "Test set: Avg. loss: 0.2907, Accuracy: 9317/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.502679\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.475130\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.510422\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.441444\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.709942\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.532564\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.717926\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.371556\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.514970\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.554398\n",
            "\n",
            "Test set: Avg. loss: 0.2781, Accuracy: 9305/10000(93%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.439187\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.591117\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.527118\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.427929\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.585121\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.656995\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.596562\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.650018\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.496015\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.481115\n",
            "\n",
            "Test set: Avg. loss: 0.2823, Accuracy: 9279/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.458709\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.564022\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.491365\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.484034\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.428327\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.501983\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.433276\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.655491\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.397122\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.647928\n",
            "\n",
            "Test set: Avg. loss: 0.2801, Accuracy: 9305/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.560210\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.485082\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.594445\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.605445\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.491437\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.518836\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.580906\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.600164\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.397404\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.476348\n",
            "\n",
            "Test set: Avg. loss: 0.2941, Accuracy: 9257/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.53e+03 ; Regulator: 77.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.89e+03 ; Regulator: 42.8 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.45e+03 ; Regulator: 19.4 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.4e+03 ; Regulator: 6.03 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.49e+03 ; Regulator: 0.657 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv2_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv2_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv2_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv2.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv2.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.91 ; output r2 = 0.95.\n",
            "nn_uniform latent r2: 0.85 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.53e+03 ; Regulator: 65.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.89e+03 ; Regulator: 28.6 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.44e+03 ; Regulator: 10.3 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.35e+03 ; Regulator: 2.48 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.34e+03 ; Regulator: 0.38 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv2_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv2_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv2_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv2.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv2.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.84 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.53e+03 ; Regulator: 42.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.89e+03 ; Regulator: 13.7 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.45e+03 ; Regulator: 4.43 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.34e+03 ; Regulator: 1.2 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.3e+03 ; Regulator: 0.253 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv2_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv2_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv2_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv2.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv2.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.79 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.82 ; output r2 = 0.87.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.53e+03 ; Regulator: 22.2 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.89e+03 ; Regulator: 6.41 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.45e+03 ; Regulator: 2.26 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.34e+03 ; Regulator: 0.737 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.3e+03 ; Regulator: 0.181 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv2_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv2_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv2_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv2.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv2.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.68 ; output r2 = 0.76.\n",
            "nn_dist latent r2: 0.75 ; output r2 = 0.81.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.53e+03 ; Regulator: 9.19 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.9e+03 ; Regulator: 2.8 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.45e+03 ; Regulator: 1.02 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.34e+03 ; Regulator: 0.358 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.3e+03 ; Regulator: 0.0972 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv2_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv2_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv2_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv2.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv2.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.38 ; output r2 = 0.46.\n",
            "nn_dist latent r2: 0.54 ; output r2 = 0.61.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv2.pkl.\n",
            "representer output r2 = -39.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 3.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3087, Accuracy: 959/10000(10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.319785\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.179907\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.561263\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.048454\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.815147\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.966368\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.855324\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.717830\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.793387\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.673286\n",
            "\n",
            "Test set: Avg. loss: 0.3954, Accuracy: 8946/10000(89%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.558120\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.559085\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.562634\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.596238\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.755328\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.660341\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.481366\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.568385\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.423330\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.688603\n",
            "\n",
            "Test set: Avg. loss: 0.3086, Accuracy: 9252/10000(93%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.337618\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.498542\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.635989\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.808650\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.591320\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.489885\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.362226\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.545508\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.711766\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.440024\n",
            "\n",
            "Test set: Avg. loss: 0.2874, Accuracy: 9309/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.514651\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.714732\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.571835\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.450693\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.650406\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.523507\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.498819\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.549889\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.507983\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.477095\n",
            "\n",
            "Test set: Avg. loss: 0.3020, Accuracy: 9258/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.481879\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.537318\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.734271\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.529140\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.496418\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.429189\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.516451\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.444325\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.576114\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.586936\n",
            "\n",
            "Test set: Avg. loss: 0.2959, Accuracy: 9285/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.551924\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.498152\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.439812\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.556922\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.678723\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.513039\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.644188\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.531427\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.611210\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.543368\n",
            "\n",
            "Test set: Avg. loss: 0.2753, Accuracy: 9335/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.421710\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.674165\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.574650\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.597739\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.509990\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.548760\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.727071\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.411438\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.530933\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.478905\n",
            "\n",
            "Test set: Avg. loss: 0.2764, Accuracy: 9360/10000(94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.530881\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.605325\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.645794\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.417153\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.426429\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.581322\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.454363\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.501034\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.408958\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.526421\n",
            "\n",
            "Test set: Avg. loss: 0.2836, Accuracy: 9317/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.498785\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.514641\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.423507\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.389233\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.499841\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.517429\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.476882\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.647925\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.529867\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.538517\n",
            "\n",
            "Test set: Avg. loss: 0.2790, Accuracy: 9347/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.610066\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.681079\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.380283\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.386928\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.603222\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.505572\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.448422\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.519468\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.612715\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.354295\n",
            "\n",
            "Test set: Avg. loss: 0.2770, Accuracy: 9376/10000(94%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.08e+03 ; Regulator: 76.2 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.3e+03 ; Regulator: 44.9 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.01e+03 ; Regulator: 21.2 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1e+03 ; Regulator: 5.66 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.08e+03 ; Regulator: 0.538 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv3_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv3_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv3_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv3.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv3.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.87 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.87 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.08e+03 ; Regulator: 62.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.31e+03 ; Regulator: 30 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1e+03 ; Regulator: 11.6 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 951 ; Regulator: 2.48 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 943 ; Regulator: 0.354 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv3_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv3_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv3_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv3.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv3.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.86 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.87 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.08e+03 ; Regulator: 40.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.31e+03 ; Regulator: 14.3 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1e+03 ; Regulator: 4.87 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 928 ; Regulator: 1.33 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 905 ; Regulator: 0.251 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv3_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv3_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv3_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv3.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv3.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.82 ; output r2 = 0.86.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.08e+03 ; Regulator: 22 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.31e+03 ; Regulator: 7.04 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.01e+03 ; Regulator: 2.62 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 926 ; Regulator: 0.863 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 898 ; Regulator: 0.19 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv3_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv3_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv3_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv3.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv3.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.69 ; output r2 = 0.74.\n",
            "nn_dist latent r2: 0.77 ; output r2 = 0.82.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.08e+03 ; Regulator: 9.73 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.31e+03 ; Regulator: 3.21 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.01e+03 ; Regulator: 1.24 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 929 ; Regulator: 0.443 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 900 ; Regulator: 0.108 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv3_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv3_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv3_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv3.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv3.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.38 ; output r2 = 0.43.\n",
            "nn_dist latent r2: 0.56 ; output r2 = 0.6.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv3.pkl.\n",
            "representer output r2 = -77.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 4.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.2973, Accuracy: 1334/10000(13%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299339\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.192123\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.219774\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.975428\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.035416\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.789419\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.739378\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.567680\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.657532\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.636792\n",
            "\n",
            "Test set: Avg. loss: 0.3889, Accuracy: 9078/10000(91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.623271\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.561191\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.475373\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.625922\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.732762\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.515863\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.686309\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.389348\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.550384\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.521392\n",
            "\n",
            "Test set: Avg. loss: 0.3091, Accuracy: 9267/10000(93%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.518133\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.526309\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.402568\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.614739\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.575573\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.549307\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.570205\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.678458\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.609325\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.727731\n",
            "\n",
            "Test set: Avg. loss: 0.3135, Accuracy: 9151/10000(92%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.294986\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.451483\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.502548\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.775937\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.563570\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.545033\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.620413\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.663284\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.485117\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.472555\n",
            "\n",
            "Test set: Avg. loss: 0.2935, Accuracy: 9332/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.575120\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.549459\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.627317\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.610135\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.673205\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.425802\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.608132\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.516826\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.771778\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.501301\n",
            "\n",
            "Test set: Avg. loss: 0.2861, Accuracy: 9327/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.725095\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.557235\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.950233\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.452422\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.368261\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.360442\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.484429\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.616137\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.451839\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.525864\n",
            "\n",
            "Test set: Avg. loss: 0.2820, Accuracy: 9347/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.633425\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.544401\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.570768\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.434038\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.603491\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.668279\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.432523\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.682796\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.508817\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.543092\n",
            "\n",
            "Test set: Avg. loss: 0.2665, Accuracy: 9355/10000(94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.581315\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.429602\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.374613\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.443078\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.360646\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.628033\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.493556\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.413963\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.567034\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.575853\n",
            "\n",
            "Test set: Avg. loss: 0.2812, Accuracy: 9303/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.600561\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.739914\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.628197\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.349464\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.539400\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.478357\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.748809\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.331431\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.499746\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.587957\n",
            "\n",
            "Test set: Avg. loss: 0.2721, Accuracy: 9330/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.397200\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.673934\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.410814\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.534041\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.414514\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.492951\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.436272\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.535775\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.467229\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.501192\n",
            "\n",
            "Test set: Avg. loss: 0.2628, Accuracy: 9378/10000(94%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 77.4 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.44e+03 ; Regulator: 42.6 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.05e+03 ; Regulator: 21 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.02e+03 ; Regulator: 6.74 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.13e+03 ; Regulator: 0.695 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv4_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv4_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv4_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv4.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv4.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.85 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 65.1 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.44e+03 ; Regulator: 28.6 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.04e+03 ; Regulator: 11.1 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 968 ; Regulator: 2.63 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 959 ; Regulator: 0.389 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv4_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv4_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv4_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv4.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv4.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.84 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 42.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.45e+03 ; Regulator: 13.5 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.04e+03 ; Regulator: 4.63 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 947 ; Regulator: 1.31 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 918 ; Regulator: 0.273 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv4_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv4_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv4_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv4.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv4.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.79 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.82 ; output r2 = 0.87.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 22.3 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.45e+03 ; Regulator: 6.6 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.05e+03 ; Regulator: 2.42 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 948 ; Regulator: 0.812 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 916 ; Regulator: 0.199 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv4_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv4_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv4_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv4.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv4.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.69 ; output r2 = 0.75.\n",
            "nn_dist latent r2: 0.76 ; output r2 = 0.82.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 9.5 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.45e+03 ; Regulator: 2.97 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.05e+03 ; Regulator: 1.13 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 951 ; Regulator: 0.406 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 918 ; Regulator: 0.109 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv4_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv4_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv4_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv4.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv4.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.4 ; output r2 = 0.46.\n",
            "nn_dist latent r2: 0.56 ; output r2 = 0.62.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv4.pkl.\n",
            "representer output r2 = -28.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 5.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3033, Accuracy: 1038/10000(10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.330379\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.166229\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.456225\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.047014\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.819300\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.734443\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.695890\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.735810\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.560060\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.668138\n",
            "\n",
            "Test set: Avg. loss: 0.3723, Accuracy: 9048/10000(90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.699610\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.524372\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.744761\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.597544\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.679240\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.475602\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.632801\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.644752\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.522543\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.508234\n",
            "\n",
            "Test set: Avg. loss: 0.3068, Accuracy: 9222/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.529190\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.439680\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.542402\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.414577\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.632786\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.500698\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.447459\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.440808\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.533407\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.654385\n",
            "\n",
            "Test set: Avg. loss: 0.2791, Accuracy: 9337/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.558895\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.527307\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.515776\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.521325\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.477436\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.518493\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.557887\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.662979\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.479684\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.545774\n",
            "\n",
            "Test set: Avg. loss: 0.2790, Accuracy: 9335/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.493053\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.327224\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.336347\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.572978\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.483302\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.630703\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.488164\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.508982\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.487834\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.596060\n",
            "\n",
            "Test set: Avg. loss: 0.2694, Accuracy: 9355/10000(94%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.384801\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.539869\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.555978\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.507785\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.561918\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.462948\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.727893\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.710722\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.504411\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.449795\n",
            "\n",
            "Test set: Avg. loss: 0.2870, Accuracy: 9325/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.605638\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.556798\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.599801\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.506633\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.581624\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.521370\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.426489\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.440068\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.577032\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.548317\n",
            "\n",
            "Test set: Avg. loss: 0.2799, Accuracy: 9354/10000(94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.594707\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.410106\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.529404\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.664464\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.633093\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.423276\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.599439\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.410055\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.544081\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.475869\n",
            "\n",
            "Test set: Avg. loss: 0.2724, Accuracy: 9403/10000(94%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.452178\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.472306\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.577825\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.694193\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.483662\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.555236\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.351600\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.473360\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.475144\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.518413\n",
            "\n",
            "Test set: Avg. loss: 0.2661, Accuracy: 9377/10000(94%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.624279\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.558421\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.527726\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.617836\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.637553\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.377673\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.515485\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.614157\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.407570\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.439872\n",
            "\n",
            "Test set: Avg. loss: 0.2880, Accuracy: 9299/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.88e+03 ; Regulator: 75.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.53e+03 ; Regulator: 41.7 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 20.4 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.14e+03 ; Regulator: 6.07 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.23e+03 ; Regulator: 0.612 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv5_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv5_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv5_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv5.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv5.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.86 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 62.6 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.53e+03 ; Regulator: 27.1 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 10.3 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.08e+03 ; Regulator: 2.52 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.07e+03 ; Regulator: 0.377 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv5_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv5_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv5_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv5.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv5.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.86 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.87 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 40.1 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.53e+03 ; Regulator: 13.1 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 4.55 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.07e+03 ; Regulator: 1.29 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.266 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv5_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv5_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv5_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv5.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv5.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.8 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.83 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.88e+03 ; Regulator: 21.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.53e+03 ; Regulator: 6.67 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 2.48 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.06e+03 ; Regulator: 0.832 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.198 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv5_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv5_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv5_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv5.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv5.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.68 ; output r2 = 0.75.\n",
            "nn_dist latent r2: 0.75 ; output r2 = 0.81.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.88e+03 ; Regulator: 9.56 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.53e+03 ; Regulator: 3.05 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 1.16 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.06e+03 ; Regulator: 0.415 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.109 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv5_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv5_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv5_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv5.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv5.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.38 ; output r2 = 0.46.\n",
            "nn_dist latent r2: 0.54 ; output r2 = 0.6.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv5.pkl.\n",
            "representer output r2 = -7.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 6.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3150, Accuracy: 981/10000(10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.269807\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.102009\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.118371\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.836491\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.025390\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.733180\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.608780\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.492143\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.695601\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.803279\n",
            "\n",
            "Test set: Avg. loss: 0.3884, Accuracy: 8993/10000(90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.709694\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.499433\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.888379\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.605196\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.603053\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.652312\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.591791\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.519505\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.759532\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.893363\n",
            "\n",
            "Test set: Avg. loss: 0.3354, Accuracy: 9172/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.745516\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.607642\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.686253\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.833511\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.632681\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.487628\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.570397\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.612162\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.829461\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.848607\n",
            "\n",
            "Test set: Avg. loss: 0.3079, Accuracy: 9269/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.623242\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.468173\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.453573\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.449782\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.833595\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.653865\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.587442\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.385258\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.559208\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.446040\n",
            "\n",
            "Test set: Avg. loss: 0.3019, Accuracy: 9262/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.482890\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.555006\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.575699\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.635003\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.572735\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.619950\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.620937\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.644414\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.573933\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.705399\n",
            "\n",
            "Test set: Avg. loss: 0.2944, Accuracy: 9312/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.620292\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.596029\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.548764\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.693785\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.367047\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.566355\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.528706\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.354801\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.523487\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.531682\n",
            "\n",
            "Test set: Avg. loss: 0.2896, Accuracy: 9308/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.529572\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.481715\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.688776\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.503740\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.793416\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.559718\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.461472\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.399720\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.467420\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.422912\n",
            "\n",
            "Test set: Avg. loss: 0.2778, Accuracy: 9353/10000(94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.589562\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.579735\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.557862\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.585669\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.462874\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.418026\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.411532\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.356780\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.704170\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.592517\n",
            "\n",
            "Test set: Avg. loss: 0.2776, Accuracy: 9379/10000(94%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.634851\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.363284\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.543000\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.376161\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.461685\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.642023\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.561242\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.498017\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.635304\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.511253\n",
            "\n",
            "Test set: Avg. loss: 0.2831, Accuracy: 9347/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.636691\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.629561\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.368154\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.580728\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.446414\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.448655\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.361362\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.551428\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.540258\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.680343\n",
            "\n",
            "Test set: Avg. loss: 0.2623, Accuracy: 9372/10000(94%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 75.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.39e+03 ; Regulator: 40.7 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.03e+03 ; Regulator: 19.7 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1e+03 ; Regulator: 5.65 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.08e+03 ; Regulator: 0.597 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv6_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv6_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv6_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv6.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv6.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.87 ; output r2 = 0.91.\n",
            "nn_dist latent r2: 0.88 ; output r2 = 0.92.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 62.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.39e+03 ; Regulator: 26.6 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.03e+03 ; Regulator: 10.3 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 949 ; Regulator: 2.5 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 934 ; Regulator: 0.391 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv6_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv6_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv6_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv6.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv6.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.87 ; output r2 = 0.91.\n",
            "nn_dist latent r2: 0.88 ; output r2 = 0.92.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 40.5 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.39e+03 ; Regulator: 12.9 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.03e+03 ; Regulator: 4.53 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 935 ; Regulator: 1.34 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 903 ; Regulator: 0.279 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv6_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv6_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv6_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv6.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv6.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.8 ; output r2 = 0.85.\n",
            "nn_dist latent r2: 0.84 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 21.3 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.39e+03 ; Regulator: 6.43 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.03e+03 ; Regulator: 2.46 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 936 ; Regulator: 0.862 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 901 ; Regulator: 0.208 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv6_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv6_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv6_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv6.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv6.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.68 ; output r2 = 0.73.\n",
            "nn_dist latent r2: 0.76 ; output r2 = 0.81.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 3.87e+03 ; Regulator: 9.43 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.4e+03 ; Regulator: 2.96 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.03e+03 ; Regulator: 1.16 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 939 ; Regulator: 0.432 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 903 ; Regulator: 0.114 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv6_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv6_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv6_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv6.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv6.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.94 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.36 ; output r2 = 0.41.\n",
            "nn_dist latent r2: 0.53 ; output r2 = 0.57.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv6.pkl.\n",
            "representer output r2 = -12.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 7.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3113, Accuracy: 1038/10000(10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305287\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.189394\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.561543\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.817529\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.706912\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.672500\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.710162\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.703018\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.454743\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.736295\n",
            "\n",
            "Test set: Avg. loss: 0.3792, Accuracy: 9075/10000(91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.647419\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.501014\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.503893\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.601292\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.597696\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.631670\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.653092\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.491745\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.524328\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.530242\n",
            "\n",
            "Test set: Avg. loss: 0.3141, Accuracy: 9210/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.393579\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.529339\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.750694\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.442283\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.655532\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.518393\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.665811\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.382168\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.736892\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.717934\n",
            "\n",
            "Test set: Avg. loss: 0.2909, Accuracy: 9283/10000(93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.709941\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.488887\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.448426\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.450391\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.487245\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.534530\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.440467\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.556111\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.606879\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.409676\n",
            "\n",
            "Test set: Avg. loss: 0.2850, Accuracy: 9270/10000(93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.596616\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.695538\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.644287\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.515486\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.458708\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.557370\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.446280\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.510042\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.581618\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.636190\n",
            "\n",
            "Test set: Avg. loss: 0.2793, Accuracy: 9356/10000(94%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.348826\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.467254\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.404184\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.641726\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.454433\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.592408\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.351860\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.626152\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.412907\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.665516\n",
            "\n",
            "Test set: Avg. loss: 0.2724, Accuracy: 9400/10000(94%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.387305\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.552538\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.536597\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.647259\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.327203\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.397595\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.578642\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.572490\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.612057\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.434220\n",
            "\n",
            "Test set: Avg. loss: 0.2768, Accuracy: 9377/10000(94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.439154\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.444573\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.636112\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.460399\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.503653\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.312334\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.460936\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.449359\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.573971\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.416075\n",
            "\n",
            "Test set: Avg. loss: 0.2723, Accuracy: 9346/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.436839\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.364757\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.367910\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.671515\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.560801\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.758329\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.478512\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.663841\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.458773\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.585432\n",
            "\n",
            "Test set: Avg. loss: 0.2651, Accuracy: 9383/10000(94%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.456709\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.468301\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.443223\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.499562\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.332125\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.404458\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.456690\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.580430\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.671696\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.508469\n",
            "\n",
            "Test set: Avg. loss: 0.2821, Accuracy: 9341/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 75.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.55e+03 ; Regulator: 38.5 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.15e+03 ; Regulator: 16.5 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.11e+03 ; Regulator: 4.62 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.17e+03 ; Regulator: 0.568 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv7_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv7_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv7_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv7.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv7.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.85 ; output r2 = 0.9.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.91.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 62.7 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.55e+03 ; Regulator: 25.2 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.15e+03 ; Regulator: 8.95 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.07e+03 ; Regulator: 2.09 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.05e+03 ; Regulator: 0.349 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv7_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv7_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv7_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv7.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv7.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.84 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 40.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.55e+03 ; Regulator: 12.4 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 4.17 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.06e+03 ; Regulator: 1.22 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.254 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv7_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv7_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv7_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv7.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv7.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.8 ; output r2 = 0.86.\n",
            "nn_dist latent r2: 0.83 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 21.6 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.56e+03 ; Regulator: 6.34 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 2.29 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.06e+03 ; Regulator: 0.783 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.189 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv7_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv7_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv7_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv7.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv7.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.67 ; output r2 = 0.74.\n",
            "nn_dist latent r2: 0.75 ; output r2 = 0.81.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.1e+03 ; Regulator: 9.32 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.56e+03 ; Regulator: 2.88 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.16e+03 ; Regulator: 1.07 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.06e+03 ; Regulator: 0.391 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.03e+03 ; Regulator: 0.103 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv7_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv7_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv7_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv7.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv7.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.97.\n",
            "nn_uniform latent r2: 0.37 ; output r2 = 0.45.\n",
            "nn_dist latent r2: 0.52 ; output r2 = 0.58.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv7.pkl.\n",
            "representer output r2 = -19.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 8.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3065, Accuracy: 1141/10000(11%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.353902\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.201565\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.670828\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.224694\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.822804\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.524782\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.614870\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.920370\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.754192\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.790684\n",
            "\n",
            "Test set: Avg. loss: 0.3870, Accuracy: 9050/10000(90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.522049\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.563123\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.641658\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.559489\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.622154\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.640005\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.711692\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.574465\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.566904\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.574875\n",
            "\n",
            "Test set: Avg. loss: 0.3344, Accuracy: 9189/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.570088\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.665641\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.752130\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.737088\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.744181\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.554104\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.544368\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.464966\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.586831\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.638151\n",
            "\n",
            "Test set: Avg. loss: 0.3280, Accuracy: 9237/10000(92%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.521944\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.637818\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.634350\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.595958\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.600066\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.442308\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.488619\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.624740\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.484766\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.500403\n",
            "\n",
            "Test set: Avg. loss: 0.3269, Accuracy: 9157/10000(92%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.620868\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.544199\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.459099\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.486993\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.400193\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.585705\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.379547\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.618543\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.599364\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.860771\n",
            "\n",
            "Test set: Avg. loss: 0.3051, Accuracy: 9247/10000(92%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.512167\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.554798\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.432543\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.512486\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.596365\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.480993\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.499740\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.605974\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.581969\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.463017\n",
            "\n",
            "Test set: Avg. loss: 0.2987, Accuracy: 9276/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.441300\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.431298\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.621281\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.546327\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.549792\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.509145\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.668044\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.455637\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.641594\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.559545\n",
            "\n",
            "Test set: Avg. loss: 0.2954, Accuracy: 9280/10000(93%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.398705\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.494898\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.556335\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.637951\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.525977\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.571625\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.485115\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.503756\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.431761\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.634032\n",
            "\n",
            "Test set: Avg. loss: 0.2786, Accuracy: 9328/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.563498\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.665685\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.616011\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.564904\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.507698\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.592631\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.446091\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.460459\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.507925\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.365843\n",
            "\n",
            "Test set: Avg. loss: 0.2882, Accuracy: 9293/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.392738\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.563456\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.454582\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.581667\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.533726\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.615987\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.518532\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.669500\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.569161\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.636944\n",
            "\n",
            "Test set: Avg. loss: 0.2907, Accuracy: 9318/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.85e+03 ; Regulator: 75.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 2.03e+03 ; Regulator: 40.9 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.59e+03 ; Regulator: 19.3 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.54e+03 ; Regulator: 6.42 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.65e+03 ; Regulator: 0.73 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv8_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv8_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv8_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv8.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv8.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.9 ; output r2 = 0.95.\n",
            "nn_uniform latent r2: 0.83 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.84 ; output r2 = 0.89.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.85e+03 ; Regulator: 62.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 2.03e+03 ; Regulator: 26.8 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.59e+03 ; Regulator: 10.4 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.5e+03 ; Regulator: 2.48 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.48e+03 ; Regulator: 0.388 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv8_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv8_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv8_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv8.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv8.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.91 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.82 ; output r2 = 0.88.\n",
            "nn_dist latent r2: 0.84 ; output r2 = 0.89.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.85e+03 ; Regulator: 40.8 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 2.04e+03 ; Regulator: 13 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.59e+03 ; Regulator: 4.54 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.47e+03 ; Regulator: 1.28 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.44e+03 ; Regulator: 0.266 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv8_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv8_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv8_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv8.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv8.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.78 ; output r2 = 0.83.\n",
            "nn_dist latent r2: 0.81 ; output r2 = 0.86.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.85e+03 ; Regulator: 22.3 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 2.04e+03 ; Regulator: 6.64 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.59e+03 ; Regulator: 2.46 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.47e+03 ; Regulator: 0.817 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.43e+03 ; Regulator: 0.196 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv8_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv8_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv8_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv8.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv8.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.64 ; output r2 = 0.71.\n",
            "nn_dist latent r2: 0.72 ; output r2 = 0.78.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.85e+03 ; Regulator: 9.48 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 2.04e+03 ; Regulator: 2.98 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.59e+03 ; Regulator: 1.13 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.48e+03 ; Regulator: 0.403 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.43e+03 ; Regulator: 0.107 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv8_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv8_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv8_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv8.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv8.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.35 ; output r2 = 0.42.\n",
            "nn_dist latent r2: 0.5 ; output r2 = 0.57.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv8.pkl.\n",
            "representer output r2 = -17.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Welcome in the approximation quality experiment for MNIST. \n",
            "Settings: random_seed = 42 ; cv = 9.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the model. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Test set: Avg. loss: 2.3127, Accuracy: 948/10000(9%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310105\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.218657\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.416991\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.024013\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.725521\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.953004\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.803707\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.647807\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.693901\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.645661\n",
            "\n",
            "Test set: Avg. loss: 0.3952, Accuracy: 9055/10000(91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.580654\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.577457\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.697988\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.550982\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.559540\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.463228\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.628641\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.820953\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.599805\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.614712\n",
            "\n",
            "Test set: Avg. loss: 0.3293, Accuracy: 9200/10000(92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.526586\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.942546\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.725284\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.556642\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.637672\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.782810\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.485267\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.697478\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.401176\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.506583\n",
            "\n",
            "Test set: Avg. loss: 0.3177, Accuracy: 9247/10000(92%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.789297\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.722910\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.452363\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.722035\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.623518\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.588214\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.550553\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.453350\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.440178\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.823749\n",
            "\n",
            "Test set: Avg. loss: 0.3082, Accuracy: 9215/10000(92%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.435733\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.404186\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.622421\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.630493\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.450428\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.548426\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.651729\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.522555\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.634040\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.510503\n",
            "\n",
            "Test set: Avg. loss: 0.3047, Accuracy: 9278/10000(93%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.584958\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.385435\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.784942\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.523782\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.492506\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.579184\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.482128\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.587618\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.520710\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.446046\n",
            "\n",
            "Test set: Avg. loss: 0.2868, Accuracy: 9313/10000(93%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.550801\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.399802\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.675766\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.440776\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.595076\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.462195\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.488518\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.389963\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.397975\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.397863\n",
            "\n",
            "Test set: Avg. loss: 0.2998, Accuracy: 9271/10000(93%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.450264\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.671515\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.624614\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.569908\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.458283\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.636220\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.821543\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.505292\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.417699\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.606120\n",
            "\n",
            "Test set: Avg. loss: 0.3016, Accuracy: 9274/10000(93%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.442699\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.642000\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.578788\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.403291\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.731171\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.563206\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.644760\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.575982\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.412096\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.728276\n",
            "\n",
            "Test set: Avg. loss: 0.2831, Accuracy: 9322/10000(93%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.829877\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.425768\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.537050\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.449084\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.583333\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.579143\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.493194\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.647181\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.464397\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.345343\n",
            "\n",
            "Test set: Avg. loss: 0.2714, Accuracy: 9338/10000(93%)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Now fitting the explainers. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "------------------------------n_keep = 3------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.59e+03 ; Regulator: 78 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.85e+03 ; Regulator: 43.9 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.39e+03 ; Regulator: 20.7 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.35e+03 ; Regulator: 5.93 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.42e+03 ; Regulator: 0.644 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv9_n3.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv9_n3.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv9_n3.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv9.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv9.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.92 ; output r2 = 0.95.\n",
            "nn_uniform latent r2: 0.86 ; output r2 = 0.89.\n",
            "nn_dist latent r2: 0.87 ; output r2 = 0.9.\n",
            "------------------------------n_keep = 5------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.59e+03 ; Regulator: 65.6 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.85e+03 ; Regulator: 29.7 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.39e+03 ; Regulator: 11.2 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.29e+03 ; Regulator: 2.63 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.27e+03 ; Regulator: 0.419 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv9_n5.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv9_n5.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv9_n5.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv9.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv9.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.85 ; output r2 = 0.88.\n",
            "nn_dist latent r2: 0.86 ; output r2 = 0.89.\n",
            "------------------------------n_keep = 10------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.59e+03 ; Regulator: 42.5 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.86e+03 ; Regulator: 14.2 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.39e+03 ; Regulator: 4.96 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.28e+03 ; Regulator: 1.41 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.24e+03 ; Regulator: 0.294 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv9_n10.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv9_n10.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv9_n10.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv9.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv9.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.82 ; output r2 = 0.86.\n",
            "nn_dist latent r2: 0.85 ; output r2 = 0.88.\n",
            "------------------------------n_keep = 20------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.59e+03 ; Regulator: 21.9 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.86e+03 ; Regulator: 6.61 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.39e+03 ; Regulator: 2.47 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.28e+03 ; Regulator: 0.838 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.24e+03 ; Regulator: 0.209 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv9_n20.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv9_n20.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv9_n20.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv9.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv9.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.72 ; output r2 = 0.77.\n",
            "nn_dist latent r2: 0.78 ; output r2 = 0.82.\n",
            "------------------------------n_keep = 50------------------------------\n",
            "Weight Fitting Epoch: 2000/10000 ; Error: 4.59e+03 ; Regulator: 9.27 ; Reg Factor: 0.398\n",
            "Weight Fitting Epoch: 4000/10000 ; Error: 1.86e+03 ; Regulator: 2.92 ; Reg Factor: 1.58\n",
            "Weight Fitting Epoch: 6000/10000 ; Error: 1.39e+03 ; Regulator: 1.12 ; Reg Factor: 6.31\n",
            "Weight Fitting Epoch: 8000/10000 ; Error: 1.28e+03 ; Regulator: 0.411 ; Reg Factor: 25.1\n",
            "Weight Fitting Epoch: 10000/10000 ; Error: 1.24e+03 ; Regulator: 0.113 ; Reg Factor: 99.9\n",
            "Saving simplex decomposition in /content/Simplex/experiments/results/mnist/quality/simplex_cv9_n50.pkl.\n",
            "Saving nn_uniform decomposition in /content/Simplex/experiments/results/mnist/quality/nn_uniform_cv9_n50.pkl.\n",
            "Saving nn_dist decomposition in /content/Simplex/experiments/results/mnist/quality/nn_dist_cv9_n50.pkl.\n",
            "Saving corpus data in /content/Simplex/experiments/results/mnist/quality/corpus_data_cv9.pkl.\n",
            "Saving test data in /content/Simplex/experiments/results/mnist/quality/test_data_cv9.pkl.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Results. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "simplex latent r2: 0.93 ; output r2 = 0.96.\n",
            "nn_uniform latent r2: 0.43 ; output r2 = 0.47.\n",
            "nn_dist latent r2: 0.59 ; output r2 = 0.62.\n",
            "Saving representer decomposition in /content/Simplex/experiments/results/mnist/quality/representer_cv9.pkl.\n",
            "representer output r2 = -22.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_bar(dists: torch.tensor, selected_idx: torch.tensor):\n",
        "  label_size = 35\n",
        "  tick_size = 25\n",
        "\n",
        "  plt.figure(figsize=(15,5))\n",
        "  plt.ylabel('Distance to test example', fontsize=tick_size)\n",
        "  plt.xticks(fontsize = tick_size)\n",
        "  plt.yticks(fontsize = tick_size)\n",
        "  plt.xlabel('Corpus examples', fontsize=tick_size)\n",
        "  \n",
        "  vals, _ = torch.sort(dists)\n",
        "  barlist = plt.bar(list(range(0,100)), vals.squeeze(0).cpu().numpy(), width=1,\n",
        "                    facecolor = '#4575b4', edgecolor='lightgrey', linewidth=0.2)\n",
        "  for idx in selected_idx:\n",
        "    barlist[idx.item()].set_color('#d73027')\n",
        "    barlist[idx.item()].set_edgecolor('lightgrey')\n",
        "  plt.savefig('hist2.pdf', format='pdf', dpi=1200)\n",
        "  plt.show()\n",
        "\n",
        "def load_run(cv: int):\n",
        "  simplex = pickle.load( open( f\"/content/Simplex/experiments/results/mnist/quality/simplex_cv{cv}_n5.pkl\", \"rb\" ) )\n",
        "\n",
        "  # load saved model\n",
        "  classifier = MnistClassifier()\n",
        "  classifier.load_state_dict(torch.load(f'/content/Simplex/experiments/results/mnist/quality/model_cv{cv}.pth'))\n",
        "  classifier.to('cuda')\n",
        "  classifier.eval()\n",
        "  return simplex, classifier\n",
        "\n",
        "simplex, classifier = load_run(0)"
      ],
      "metadata": {
        "id": "fTmpWCPzxfRP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test example i which examples in the corpus have weight > 0.01\n",
        "i = 0\n",
        "test_i_lrg_corpus_weights = simplex.weights[i] > 0.01"
      ],
      "metadata": {
        "id": "KXX_2YKg1IWl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the latent state of the test example\n",
        "test_example = simplex.test_examples[i].unsqueeze(0)\n",
        "test_example_latent_true = classifier.latent_representation(test_example)\n",
        "\n",
        "# this is equivelent and returns the same vector\n",
        "test_example_latent_true = simplex.test_latent_reps[i].unsqueeze(0)\n",
        "\n",
        "# now grab the approximation of that vector in the corpus convex hull\n",
        "test_latent_approx = simplex.latent_approx()\n",
        "test_example_latent_approx = test_latent_approx[i].unsqueeze(0)\n",
        "\n",
        "# distance between true latent and approx latent of test example\n",
        "approx_dist = torch.sum((test_example_latent_approx - test_example_latent_true)**2)**0.5\n",
        "print('Distance between latent approx and latent true:', approx_dist.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3myAzfeXXPVf",
        "outputId": "1e4ca4a8-ec37-48d0-d01d-7f80dfbfbdbc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between latent approx and latent true: 5.254690170288086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate distance from test latent approximation to the highly weighted corpus examples\n",
        "dists = torch.cdist(test_example_latent_approx, simplex.corpus_latent_reps)\n",
        "_, ranks = torch.unique(dists, sorted=True, return_inverse=True)\n",
        "sorted_ranks, _ = ranks[0,test_i_lrg_corpus_weights].sort()\n",
        "\n",
        "\n",
        "print('Latent test representation distance to each corpus representation is calcuated and ordered (low to high).')\n",
        "print('Then the rank of the corpus examples used in the reconstruction is extracted.\\n')\n",
        "print('Approximate latent representation of the test example:', sorted_ranks)\n",
        "vals, idxs = torch.sort(dists)\n",
        "weights = simplex.weights[i][idxs[0]][test_i_lrg_corpus_weights[idxs[0]]]\n",
        "print('With corresponding weight values: ', weights)\n",
        "plot_bar(dists, ranks[0,test_i_lrg_corpus_weights])\n",
        "\n",
        "\n",
        "# calculate distance from test latent ground truth to the highly weighted corpus examples\n",
        "dists = torch.cdist(test_example_latent_true, simplex.corpus_latent_reps)\n",
        "_, ranks = torch.unique(dists, sorted=True, return_inverse=True)\n",
        "sorted_ranks, _ = ranks[0,test_i_lrg_corpus_weights].sort()\n",
        "\n",
        "print('\\nTrue latent representation of the test example:', sorted_ranks)\n",
        "vals, idxs = torch.sort(dists)\n",
        "weights = simplex.weights[i][idxs[0]][test_i_lrg_corpus_weights[idxs[0]]]\n",
        "print('With corresponding weight values: ', weights)\n",
        "\n",
        "# plot_bar(dists, ranks[0,test_i_lrg_corpus_weights])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "pzKT3KIXsML8",
        "outputId": "afc9723c-dd5d-4a78-f009-6f943f93602c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latent test representation distance to each corpus representation is calcuated and ordered (low to high).\n",
            "Then the rank of the corpus examples used in the reconstruction is extracted.\n",
            "\n",
            "Approximate latent representation of the test example: tensor([ 1,  2, 68], device='cuda:0')\n",
            "With corresponding weight values:  tensor([0.3299, 0.4351, 0.2263], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "True latent representation of the test example: tensor([ 1,  2, 55], device='cuda:0')\n",
            "With corresponding weight values:  tensor([0.3299, 0.4351, 0.2263], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T8b9CidE_dr2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def reconstruction_rank_dist(i, simplex):\n",
        "  \"\"\"\n",
        "  Of the latent corpus examples used to approximate the test example with\n",
        "  weight > 0.01, find and return their rank distance to the original latent test\n",
        "  example of the 1000 total corpus examples and also return their associated \n",
        "  weights.\n",
        "  \"\"\"\n",
        "  test_i_lrg_corpus_weights = simplex.weights[i] > 0.01\n",
        "\n",
        "  # this is equivelent and returns the same vector\n",
        "  test_example_latent_true = simplex.test_latent_reps[i].unsqueeze(0)\n",
        "\n",
        "  # now grab the approximation of that vector in the corpus convex hull\n",
        "  test_latent_approx = simplex.latent_approx()\n",
        "  test_example_latent_approx = test_latent_approx[i].unsqueeze(0)\n",
        "\n",
        "  # calculate distance from test latent approximation to the highly weighted corpus examples\n",
        "  dists = torch.cdist(test_example_latent_approx, simplex.corpus_latent_reps)\n",
        "  _, ranks = torch.unique(dists, sorted=True, return_inverse=True)\n",
        "  sorted_ranks, _ = ranks[0,test_i_lrg_corpus_weights].sort()\n",
        "\n",
        "  # calculate distance from test latent ground truth to the highly weighted corpus examples\n",
        "  dists = torch.cdist(test_example_latent_true, simplex.corpus_latent_reps)\n",
        "  _, ranks = torch.unique(dists, sorted=True, return_inverse=True)\n",
        "  sorted_ranks, _ = ranks[0,test_i_lrg_corpus_weights].sort()\n",
        "\n",
        "  vals, idxs = torch.sort(dists)\n",
        "  weights = simplex.weights[i][idxs[0]][test_i_lrg_corpus_weights[idxs[0]]]\n",
        "  return sorted_ranks, weights\n",
        "\n",
        "def calculate_for_test(simplex):\n",
        "  N_TEST = 100\n",
        "  weighted_ranks = torch.empty(0, device='cuda')\n",
        "  all_ranks = torch.empty(0, device='cuda')\n",
        "  for i in range(N_TEST):\n",
        "    sorted_ranks, weights = reconstruction_rank_dist(i, simplex)\n",
        "    all_ranks = torch.cat([all_ranks, sorted_ranks])\n",
        "    weighted_rank_i = sorted_ranks * weights\n",
        "    weighted_ranks = torch.cat([weighted_ranks,weighted_rank_i])\n",
        "  return weighted_ranks, all_ranks"
      ],
      "metadata": {
        "id": "AzNLGOjnnCSv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_test = 100\n",
        "avg_weighted_ranks = []\n",
        "avg_avg_ranks = []\n",
        "all_ranks_full = []\n",
        "for cv in cv_ls:\n",
        "  simplex, classifier = load_run(cv)\n",
        "  weighted_ranks, all_ranks = calculate_for_test(simplex)\n",
        "  weighted_rank = weighted_ranks.sum()/n_test\n",
        "  avg_rank = torch.mean(all_ranks)\n",
        "  print('Weighted rank:', weighted_rank.item())\n",
        "  print('Raw average rank: ', avg_rank.item())\n",
        "  avg_weighted_ranks.append(weighted_rank.item())\n",
        "  avg_avg_ranks.append(avg_rank.item())\n",
        "  all_ranks_full.extend(all_ranks.cpu().tolist())\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjGL2rAMnCV7",
        "outputId": "111cf341-1d08-417b-b8aa-652777a595d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted rank: 4.6459269523620605\n",
            "Raw average rank:  9.16464900970459\n",
            "Weighted rank: 4.556447505950928\n",
            "Raw average rank:  8.503649711608887\n",
            "Weighted rank: 4.700008392333984\n",
            "Raw average rank:  8.49400520324707\n",
            "Weighted rank: 4.27772855758667\n",
            "Raw average rank:  6.944711685180664\n",
            "Weighted rank: 4.627486705780029\n",
            "Raw average rank:  7.525179862976074\n",
            "Weighted rank: 4.2587409019470215\n",
            "Raw average rank:  7.700460910797119\n",
            "Weighted rank: 4.835953235626221\n",
            "Raw average rank:  8.122352600097656\n",
            "Weighted rank: 4.5679521560668945\n",
            "Raw average rank:  9.148882865905762\n",
            "Weighted rank: 4.485421180725098\n",
            "Raw average rank:  8.66416072845459\n",
            "Weighted rank: 3.627946138381958\n",
            "Raw average rank:  6.056234836578369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7qi-DkdZ0JUG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(avg_weighted_ranks))\n",
        "print(np.std(avg_weighted_ranks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnULl3XLlJnP",
        "outputId": "9dbc7f5b-3987-4d19-eacf-124853c2f72c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.458361172676087\n",
            "0.32391085072615516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(avg_avg_ranks))\n",
        "print(np.std(avg_avg_ranks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UMgXOs8lM1_",
        "outputId": "cde1f69f-61fa-4da4-9d4d-0226d51ff2a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.032428741455078\n",
            "0.9395020486339911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weighted rank:', weighted_ranks.sum()/n_test)\n",
        "print('Raw average rank: ', torch.mean(all_ranks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVlYn_ddnCYf",
        "outputId": "c88c628f-83aa-45ea-cebd-e45cd6026663"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted rank: tensor(3.6279, device='cuda:0')\n",
            "Raw average rank:  tensor(6.0562, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((np.array(all_ranks_full) > 10).sum()/len(all_ranks_full))\n",
        "print((np.array(all_ranks_full) > 20).sum()/len(all_ranks_full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o5bFBJQrSGd",
        "outputId": "7efcf03a-9a04-447b-b125-eae5fb4acd5d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22055984555984556\n",
            "0.11341698841698841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_size = 35\n",
        "tick_size = 25\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.hist(all_ranks_full, bins=20, facecolor = '#4575b4', edgecolor='white',\n",
        "         linewidth=1, density=True)\n",
        "plt.ylabel('Frequency', fontsize=tick_size)\n",
        "plt.xticks(fontsize = tick_size)\n",
        "plt.yticks(fontsize = tick_size)\n",
        "plt.xlabel('Rank distance of corpus decomposition examples', fontsize=tick_size)\n",
        "plt.savefig('hist.pdf', format='pdf', dpi=1200)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "lB4LxBuIqagY",
        "outputId": "48c24a2a-c635-4418-e5fd-f8f1c69b4017"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oWivW48vnCaZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ftz8j07lie_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OMrh-3h02GsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BFZ9l-zxJgLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BD-dsBTtJgOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rOyziiD2JgRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e4x0QMXqIrUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_SYn0sKpBciG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}